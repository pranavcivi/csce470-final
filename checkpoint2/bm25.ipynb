{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\civip\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from collections import Counter\n",
    "import pickle\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from lyricsgenius import Genius\n",
    "\n",
    "with open('../lyrics-updated.pkl', 'rb') as f:\n",
    "    song_lyrics = pickle.load(f)\n",
    "\n",
    "\n",
    "with open('../classifications.pkl', 'rb') as f1:\n",
    "    classifications = pickle.load(f1)\n",
    "\n",
    "with open('../big_songs.pkl', 'rb') as f2:\n",
    "    big_songs = pickle.load(f2)\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "def tokenize(lyrics):\n",
    "    words = lyrics.lower().split()\n",
    "    return [word for word in words if word not in stop_words]\n",
    "\n",
    "k1 = 1.0\n",
    "b = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for \"I Hate Everything About You\" by Three Days Grace...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# call the api to get the lyrics\n",
    "\n",
    "genius = Genius('HhcPCGMwENzdfMQlauEV1Om0UormkWc6Wrwxri-LFTfBSNkzhwMSVUnR5tZG8eXW')\n",
    "genius.remove_section_headers = True\n",
    "genius.skip_non_songs = False\n",
    "genius.excluded_terms = [\"Remix\", \"Live\"]\n",
    "\n",
    "# song_name = \"Happy\"\n",
    "# artist = \"Pharrel Williams\"\n",
    "# song_name = \"Firework\"\n",
    "# artist = \"Katy Perry\"\n",
    "song_name = \"I Hate Everything About You\"\n",
    "artist = \"Three Days Grace\"\n",
    "\n",
    "song = genius.search_song(song_name, artist)\n",
    "pos = song.lyrics.find(f'{song_name} Lyrics')\n",
    "query = song.lyrics[pos:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i hate everything about you lyrics every time we lie awake after every hit we take every feeling that i get but i havent missed you yet every roommate kept awake by every sigh and scream we make all the feelings that i get but i still dont miss you yet  only when i stop to think about it  i hate everything about you why do i love you i hate everything about you why do i love you every time we lie awake after every hit we take every feeling that i get but i havent missed you yet  only when i stop to think about it you might also like i hate everything about you why do i love you i hate everything about you why do i love you  only when i stop to think about you i know only when you stop to think about me do you know  i hate everything about you why do i love you you hate everything about me why do you love me i hate you hate i hate you love me i hate everything about you why do i love youembed'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# then remove  capital letters and non alphanumeric characters\n",
    "query = query.lower()\n",
    "query = ''.join([char if char.isalpha() or char.isspace() else '' for char in query.replace('\\n', ' ')])\n",
    "# it might seem crazy what im bout to say\n",
    "query\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize all documents\n",
    "tokenized_songs = {}\n",
    "for category, all_lyrics in big_songs.items():\n",
    "    tokenized_songs[category] = tokenize(all_lyrics)\n",
    "\n",
    "# calculate document lengths\n",
    "doc_lengths = {}\n",
    "for  category, all_lyrics in tokenized_songs.items():\n",
    "    doc_lengths[category] = len(all_lyrics)\n",
    "\n",
    "# avg doc length\n",
    "avg_doc_length = sum(doc_lengths.values()) / len(doc_lengths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate doc freqs\n",
    "doc_freqs = {}\n",
    "for doc in tokenized_songs.values():\n",
    "    unique_terms = set(doc)\n",
    "    for term in unique_terms:\n",
    "        doc_freqs[term] = doc_freqs.get(term, 0) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bm25 algorithm\n",
    "def bm25(query, document, doc_length):\n",
    "    score = 0.0\n",
    "    query_terms = tokenize(query)\n",
    "    term_freqs = Counter(document)\n",
    "    \n",
    "    for term in query_terms:\n",
    "        if term in doc_freqs:  # term must appear in the corpus\n",
    "            # Calculate BM25 components\n",
    "            doc_freq = doc_freqs[term]\n",
    "            idf = math.log((len(big_songs) - doc_freq + 0.5) / (doc_freq + 0.5) + 1)\n",
    "            term_freq = term_freqs[term]\n",
    "            term_score = idf * (term_freq * (k1 + 1)) / (term_freq + k1 * (1 - b + b * doc_length / avg_doc_length))\n",
    "            score += term_score\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document Rankings:\n",
      "Rank 1: Document uplifting with score 70.0821\n",
      "Rank 2: Document dark with score 69.3581\n",
      "Rank 3: Document energetic with score 66.3306\n",
      "Rank 4: Document dynamic with score 60.0821\n",
      "Rank 5: Document anger with score 59.5034\n",
      "Rank 6: Document fierce with score 58.2192\n",
      "Rank 7: Document cynical with score 6.2137\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "for doc_id, doc in tokenized_songs.items():\n",
    "    score = bm25(query, doc, doc_lengths[doc_id])\n",
    "    scores.append((doc_id, score))\n",
    "\n",
    "# Sort documents by score in descending order\n",
    "ranked_docs = sorted(scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Display the ranking results\n",
    "print(\"Document Rankings:\")\n",
    "for rank, (doc_id, score) in enumerate(ranked_docs, start=1):\n",
    "    print(f\"Rank {rank}: Document {doc_id} with score {score:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
